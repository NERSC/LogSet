# An ontology for a dataset of system logs and related information to support 
# failure analysis for large HPC clusters

# Ontology scope and competency questions:
#  - purpose of this ontology is to allow logs relevant to some HPC system
#    (primarily targetting Cray systems at DOE sites, but the idea is more
#    generally valid). It should support publication of:
#    1. a data dictionary - which describes "types" of log such that a tool 
#       can read/make use of them (eg a console log for a cray XC with CNL6 
#       such as NERSC Edison)
#    2. an entity catalog, identifying/describing systems and components and 
#       how they affect each other (eg edison scratch affects edison, so does 
#       the NGF shared filesystem, so does the cooling system in the room 
#       housing them) 
#    3. an index for a given LogSet. A LogSet is a set of related logs bundled 
#       together with a common subject and common (perhaps aggregate) timespan.
#       Each piece of a LogSet has a LogSeries (the "type" of log, ie how a 
#       tool can handle it) and a dcat:Distribution 
#       The index lists the parts of a LogSet as logset:ConcreteLogs, a 
#       constrained type of dcat:Distribution, thus describing how to access 
#       the file/stram/api/whatever.
#       A query for logs relevant to some event might return data from 
#       several LogSets, eg the SMW system logs, the Slurm history, the 
#       filesystem logs, the MOTDs, and annotations of known events affecting 
#       that system during the period of interest
#    4. Annotations. A set of annotations might span multiple LogSets, and 
#       a LogSet may be the subject of several annotation sets
#       .. Annotations are a type of LogSeries really..

#  - ontology describes log data for an HPC cluster (primarily target Cray
#    systems at DOE sites, but ideally more generally valid). The LogSet itself
#    is a dcat:DataSet, access to it is expected to be published as a 
#    dcat:Distribution and by an organization described in terms of foaf:Agent.
#  - 
#    The system in question is described as an adms:Asset.
#  - the specifics added by this ontology are intended to support tools that 
#    collate and manipulate related sets of logs, for the purpose of 
#    investigating resiliency. Especially, logs and system-state data are 
#    stored as timestamped records of events or state, usually in some defined
#    format (eg record-per-line-in-order-of-increasing time), and often with 
#    some component-identifying information. Thus a tool should be able to use 
#    an RDF description of a LogSet to select the approprate class to 
#    instantiate to manipulate each file or data source in the LogSet
#  - A LogSet has associated Annotations: human- or tool-generated descriptions
#    of events with a timespan and links to possibly-relevraent log entries
#  - Therefore, competency questions are:
#     - which logs are relevant to system xyz during time period abc?
#     - which log entries are relevant to thhis annotation?
#     - what file and record format are used in a selected logfile?
#     - how should a tool access this LogSeries in this LogSet?
#    (some more general competency questions - more at the dcat:Dataset level
#    are eg "what LogSets are available for system xyz?")

# empty prefix for "this ontology" - replace the URL eventually:
#@prefix : <http://example.com/owl/hpc-logset/> .
# what if I use the github location to get a published version?
@prefix : <https://raw.githubusercontent.com/NERSC/LogSet/master/etc/vocab#> .

# the dataset definition is derived from the Data Catalog Vocabulary, a single
# dataset is a subtype of dcat:Dataset and can be managed as part of a 
# dcat:Catalog and published/accessed as a dcat:Distribution
@prefix dcat: <http://www.w3.org/ns/dcat#> .

# basic vocabularies used here:
# rdf and rdfs between them define essential types and relationships:
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
# owl adds a more constrained formal description logic for classes:
@prefix owl: <http://www.w3.org/2002/07/owl#> .
# dublin core terms, form base of dcat vocab:
@prefix dct: <http://purl.org/dc/terms/> .
# For provenance information: the publisher of a LogSet is a foaf:Agent (most
# likely an foaf:Organization), and the LogSet is comprised of logfiles for one
# or more systems, described in terms of organizational Assets (adms:Asset):
@prefix foaf: <http://xmlns.com/foaf/0.1/> .
@prefix adms: <http://www.w3.org/ns/adms#> .

# to compare dateTime types:
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .

# do I need and equivalent of this?
#<http://www.w3.org/ns/dcat>
#    a owl:Ontology, voaf:Vocabulary;
#    rdfs:label "The data catalog vocabulary"@en;
#    .

:subject 
    a owl:ObjectProperty ;
    # this is kind of an ugly way to do it, might want to rethink it (eg 
    # by making a common base class for LogSet and LogSeries)
    rdfs:domain [ owl:UnionOf (:LogSet :LogSeries) ] ;
    rdfs:range adms:Asset ;
    rdfs:comment ''' A LogSet has logs about a specific system or component,
                     and a LogSeries might be about a specific component 
                     within the system the LogSet documents 
                 ''' ;
    .


:affects
    a owl:ObjectProperty ;
    rdfs:comment ''' Components in an HPC center are not a strict hierarchy,
                     eg a filesystem might be shared by multiple systems. 
                     So we connect related systems and subsystems with an 
                     "affects" relationship
                 ''' ;
    rdfs:domain adms:Asset ;
    rdfs:range adms:Asset ;
    .


# make dct:temporal amenable to sparql queries, following the pattern used by
# DDI Alliance and OpenGeoSpatial.org of defining startDate and endDate properties
# for a dct:PeriodOfTime, and in indexes and queries use a nested blank node for
# the dct:PeriodOfTime itself.
# Note that for sparql queries to work, the dates *must* be include a timezone
# and be marked as ^^xsd:dateTime, both in the index and in the query (or they
# get compared as literals). The time part (eg T12:00:00) is optional
:startDate
    a owl:DatatypeProperty ;
    rdfs:domain dct:PeriodOfTime ;
    rdfs:range xsd:dateTime ;
    .

:endDate
    a owl:DatatypeProperty ;
    rdfs:domain dct:PeriodOfTime;
    rdfs:range xsd:dateTime ;
    .


:LogSet
    a owl:Class ;
    rdfs:subClassOf dcat:Dataset ;
    rdfs:subClassOf [
        rdf:type owl:Restriction ;
        owl:onProperty dct:temporal ; 
          # eg:"start=2016-01-01T12:00:00+08:00 end=2016-01-05T14:45:00+08:00"
        owl:cardinality 1 ;
    ] ;
    rdfs:subClassOf [
        rdf:type owl:Restriction ;
        owl:onProperty :subject ; 
          # eg: <http://portal.nersc.gov/project/m888/resilience/entities/#edison
    ] ;
    rdfs:comment ''' A LogSet is a Dataset for a specific system, spanning a 
                     specific timeframe (eg a Cray boot session on NERSC Cori). 
                     It is recommended but not enforced that a LogSet instance 
                     should have certain properties describing its provenance:   
                     # dct:title         a rdfs:Literal
                     # dct:description   a rdfs:Literal
                     # dct:publisher     a foaf:Organization
                     # dct:contactPoint  a vcard:Kind
                     The actual files (or db queries etc) of the logs are 
                     described as a :ConcreteLog, based on dcat:Distribution
                 ''' ;
    .

:LogSeries
    a owl:Class;
    rdfs:comment ''' A "type" of logfile, eg a console or outage log. Can be 
                     used by tools to indicate how to handle a log. Must have
                     a property indicating the "canonical" format that tools
                     can assume on
                     hmm: if we have multiple source for same type of log (eg 
                     version in elasticsearch), then maybe we need an 
                     "equivalentTo" property to link them?
                 ''' ;
    rdfs:subClassOf [
        rdf:type owl:Restriction ;
        owl:onProperty :logFormat ;
        owl:cardinality 1 ;
    ] ;
    .

:LogFormatType
    a owl:Class ;
    rdfs:comment ''' An entity representing how log data is organized, in order 
                     to help tools decide how to handle a give log.  
                     For example, most logs are timestamped-line-per-entry but
                     some are a timestamped multiline block, others have a file
                     per timestamp and contain a table, others hove no obvious 
                     structure at all. 
                     A LogFormatType is expected to correspond to a handler 
                     class for logs in that format
                 ''' ;
    .

:logFormat
    a owl:ObjectProperty, owl:FunctionalProperty ;
    rdfs:comment ''' An entity indicating how a LogSeries is formatted, from 
                     which tools can decide how to process it.
                     For example, most logs are timestamped-line-per-entry (in
                     a file or stream) but some are a timestamped multiline 
                     block, others have a file per timestamp and contain a 
                     table, others hove no obvious structure at all. 
                     problem: what if the 
                     same log might be in a native file or as records in a 
                     database? then the logFormat should provide enough 
                     information for a tool to answer questions about it?
                     or does that count as a different logseries?
                 ''' ;
    rdfs:domain :LogSeries ;
    rdfs:range :LogFormatType ;
    .


:logFormatInfo
    a owl:DatatypeProperty ;
    rdfs:comment ''' dictionary-like information that a LogFormatType handler 
                     class can use to interpret a specific type of logfile. Each 
                     logFormatInfo property should be a literal like 
                     "key=value". Eg, a console_logfile and message_logfile are
                     both timeStampedLogFile, but in the console log the 
                     timestamp is the first "word" on each line, while in the 
                     message log the timestamp is the second word on each line.
                     So console_logfile can have recordInfoProperty "ts_word=0"
                     (and "part_word=1" for system component) 
                 ''' ;
    rdfs:domain :LogSeries ;
    rdfs:range rdfs:Literal ;
    .


:ConcreteLog
    a owl:Class ;
    rdfs:subClassOf dcat:Distribution ;
    # required properties:
    #rdfs:subClassOf [
    #    # actually is this one strictly required? the isInstanceOf might cover it
    #    rdf:type owl:Restriction ;
    #    owl:onProperty dcat:mediaType ; # eg "application/json" or "text/plain"
    #    owl:cardinality 1 ;
    #] ;
    rdfs:subClassOf [
        rdf:type owl:Restriction ;
        owl:onProperty :isInstanceOf ; # eg dict:console_logfile
        owl:cardinality 1 ;
    ] ;
    rdfs:comment ''' storage of a specific log, eg a File or database or web 
                     service, with knowledge of how to handle this type of 
                     log in this format. Eg, the actual files in a p0 package
                     are ConcreteLog objects 
                 ''' ;
    .


:isInstanceOf
    a owl:ObjectProperty, owl:FunctionalProperty ;
    rdfs:comment "Links a LogSeries to log data (eg a log file)" ;
    #rdfs:domain :DataSource ;
    rdfs:domain :ConcreteLog ;
    rdfs:range :LogSeries ;
    .

# this might also be extraneous, just use dcat:Distribution
#:File
#    a owl:Class ;
#    rdfs:subClassOf :DataSource ; 
#    # required properties:
#    rdfs:subClassOf [
#        rdf:type owl:Restriction ;
#        owl:onProperty :path ; # eg "/abs/path/to/file" or "../rel/path/to/file"
#        owl:cardinality 1 ;
#    ] ;
#    .
#
#:path
#    a owl:DatatypeProperty ;
#    rdfs:comment "path relative to the index.ttl file" ;
#    rdfs:domain :File ;
#    rdfs:range rdfs:Literal ;
#    .
#
#:hasLogData
#    a owl:ObjectProperty ;
#    rdfs:comment "Links log data (eg a log file) to a LogSet" ;
#    rdfs:domain :LogSet ;
#    rdfs:range :DataSource ;
#    .

# I think it needs to be the other way around..
#:hasAnnotations
#    a owl:ObjectProperty ;
#    rdfs:comment "Links annotation data to a LogSet" ;
#    rdfs:domain :LogSet ;
#    rdfs:range :DataSource ;
#    .

# =========


## -------
#
# some example things within a dataset:
#
# BEGIN=p0-<timestamp> file. Empty file, marks start of boot session
#
# bootinfo.p0-20170906t151820. seems to have output of commands, delimited by 
#                              start-of-line timestamps. Some output lines are 
#                              also prefixed by timestamp, but not all
# bootrecord/boothistory       timestamped log  
#           /boot_record       blocks of text, each coresponding to a log entry
#           /node_table        some sort of table
# bootrecorder-<datestamp>     timestamped log
# compute/<nodename>-<datestamp> timestamped log
# config.p0-<timestamp>        unstructured text
# console-<datestamp>          timestamped log
# console.pid.p0               unstructured text
# console.p0-20170906t151820   empty file
# console.cmd.p0               unstructured text
# consumer-<datestamp>         timestamped blocks of text               
# craylog.p0-20170906t151820   multiple sections, including tables, logs and unstructured text
# craylog.p0-<timestamp>.save* appears to be manually captured state of main craylog
#                              at a couple of points in time. Not part of each dataset
# dumpd-<datestamp>            timestamped log
# dws/dwmd-<datestamp>         timestamped log
# hwerrlog.p0-20170906t151820            binary data
# hwerrlog.p0-20170906t151820.<number>   binary data
# hwinv.p0-20170906t151820     xml 
# messages-<datestamp>         timestamped log
# nersc.aries_lcb/<cluster>_aries_lcb_status.<timestamp>.xz table
# netwatch-<datestamp>         table with timestamp columns
# nlrd-<datestamp>             timestamped log
# etc
#
# will also need to have annotations db, and event log (eg outages), and perhaps job info
# (maybe a db, or log, or both)
#
# so file types will need to reflect this.
# maybe each thing has a label, and a rule for the filename pattern

