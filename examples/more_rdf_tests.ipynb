{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadSyntax",
     "evalue": "at line 14 of <>:\nBad syntax (Prefix \"logset:\" not bound) at ^ in:\n\"...b'elf and set a prefix:\\n@prefix : <examples/index#> .\\n:\\n    a '^b'logset:LogSet ;\\n    dct:description \"a partial sample of cor'...\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32munknown\u001b[0m\n\u001b[0;31mBadSyntax\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/global/homes/s/sleak/.local/cori/3.6-anaconda-4.4/lib/python3.6/site-packages')\n",
    "import rdflib\n",
    "\n",
    "#sys.path.append('/global/homes/s/sleak/Monitoring/Resilience/LogSet/prototypes')\n",
    "#import Graph    \n",
    "#\n",
    "## force re-read of Graph.py in case I've edited it:\n",
    "#import importlib\n",
    "#importlib.reload(Graph)\n",
    "#\n",
    "#url = 'nersc-catalog.ttl'\n",
    "#vocab = '../etc/vocab.ttl'\n",
    "##Graph.read(url, vocab=vocab)\n",
    "#Graph.construct(url, local_only=False)\n",
    "#\n",
    "#Graph.dump()\n",
    "def dump(graph):\n",
    "    print(graph.serialize(format='n3').decode('ascii'))\n",
    "\n",
    "import urllib.request\n",
    "def parse_line_by_line(url, fmt):\n",
    "    \"\"\" rdflib is really uninformative about the location of the error\n",
    "        when it can't parse a file, so use this to parse line-by-line \n",
    "        and look for errors\n",
    "        (hmm, does not work, fails on valid files)\n",
    "    \"\"\"\n",
    "    f = urllib.request.urlopen(url)\n",
    "    g = rdflib.ConjunctiveGraph()\n",
    "    linecount=0\n",
    "    for line in f.readlines():\n",
    "        linecount += 1\n",
    "        print(\"parsing line {0:d}: {1}\".format(linecount,line))\n",
    "        rdftext = line.strip()\n",
    "        if not rdftext: continue\n",
    "        try:\n",
    "            g.parse(data=rdftext, format=fmt)\n",
    "        except:\n",
    "            print(\"failed to parse line {0:d} of {1}\".format(linecount,url))\n",
    "            print(line)\n",
    "            print(g.serialize())\n",
    "            raise\n",
    "    print(\"finished parsing {0:s} successfully!\".format(url))\n",
    "    \n",
    "import os\n",
    "def construct(*catalog_urls, spider=False):\n",
    "    graph = rdflib.ConjunctiveGraph()\n",
    "    unparsed = set(catalog_urls)\n",
    "    print(unparsed)\n",
    "    parsed = set()\n",
    "    q_peers = 'SELECT ?uri WHERE { ?cat logset:peers ?uri . }'\n",
    "    while len(unparsed) > 0:\n",
    "        for url in unparsed:\n",
    "            print(\"current dir is\" + os.getcwd())\n",
    "            print(url)\n",
    "            if os.path.isfile(url):\n",
    "                print(\"yes, found \" + str(url))\n",
    "            else:\n",
    "                print(\"no, did not find \" + str(url))\n",
    "            fmt = rdflib.util.guess_format(url)\n",
    "            \n",
    "            parse_line_by_line(url, format)\n",
    "            graph.parse(url, format=fmt)\n",
    "            \n",
    "            \n",
    "                \n",
    "            parsed.add(url)\n",
    "        unparsed = set()   \n",
    "        if spider:\n",
    "            for peer in graph.query(q_peers):\n",
    "                url = peer['uri']\n",
    "                if not url in parsed:\n",
    "                    unparsed.add(url)\n",
    "                    \n",
    "    q_logsets = ''' SELECT ?uri \n",
    "                    WHERE {\n",
    "                        ?cat dcat:dataset ?uri . \n",
    "                    }'''\n",
    "    for logset in graph.query(q_logsets):\n",
    "        print(\"found a logset url: {0}\".format(str(logset)))\n",
    "        url = logset['uri']\n",
    "        fmt = rdflib.util.guess_format(url)\n",
    "        graph.parse(url, format=fmt)\n",
    "    return graph\n",
    "\n",
    "# quick sanity check:\n",
    "#parse_line_by_line('file:/global/u1/s/sleak/Monitoring/Resilience/LogSet/examples/nersc-entities.ttl', 'turtle')\n",
    "g1 = rdflib.ConjunctiveGraph().parse(\"file:/global/u1/s/sleak/Monitoring/Resilience/LogSet/examples/index.ttl\", format='turtle')\n",
    "print(\"got this far...\")\n",
    "print(g1.serialize(format='n3').decode('ascii'))\n",
    "\n",
    "graph = construct('nersc-entities.ttl', spider=True)\n",
    "dump(graph)\n",
    "                \n",
    "    \n",
    "\n",
    "[ns for ns in graph.namespaces()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-36f1fafedb25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#while\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mspider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mspider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'graph' is not defined"
     ]
    }
   ],
   "source": [
    "# find other catalogs\n",
    "def spider(graph):\n",
    "    q_cats = '''SELECT ?uri\n",
    "                 WHERE {\n",
    "                    ?cat a dcat:Catalog .\n",
    "             '''\n",
    "    q_peers = '''SELECT ?uri\n",
    "                 WHERE {\n",
    "                    ?cat a dcat:Catalog .\n",
    "                    ?cat logset:peers ?uri .\n",
    "                 } '''\n",
    "    for peer in graph.query(q_peers):\n",
    "        print(peer['uri'])\n",
    "        graph.parse(peer['uri'], format=rdflib.util.guess_format(peer['uri']))\n",
    "        \n",
    "#while \n",
    "spider(graph)\n",
    "dump(graph)\n",
    "spider(graph)\n",
    "dump(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find datasets, read them, and if they are LogSets, add them to the graph:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
